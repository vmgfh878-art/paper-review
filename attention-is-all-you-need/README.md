# ğŸ“„ Paper Review: Attention Is All You Need

> Vaswani et al., NIPS 2017  
> ë°œí‘œì: ê¹€ì§€í˜• (ì„¸ì¢…ëŒ€í•™êµ)

---

## ğŸ“Œ ë…¼ë¬¸ ê°œìš”

| í•­ëª© | ë‚´ìš© |
|------|------|
| ì œëª© | Attention Is All You Need |
| ì €ì | Vaswani et al. |
| í•™íšŒ | NIPS 2017 |
| í•µì‹¬ ì•„ì´ë””ì–´ | Recurrence ì—†ì´ Attentionë§Œìœ¼ë¡œ Seq2Seq ìˆ˜í–‰ |

---

## ğŸ§  Introduction: ì™œ Transformerì¸ê°€?

ê¸°ì¡´ NLPëŠ” RNN/LSTM ê¸°ë°˜ Seq2Seq ëª¨ë¸ì´ ì§€ë°°ì ì´ì—ˆìœ¼ë‚˜ ë‘ ê°€ì§€ ê·¼ë³¸ì ì¸ í•œê³„ê°€ ìˆì—ˆìŠµë‹ˆë‹¤.

- **ìˆœì°¨ ì²˜ë¦¬(Sequential computation)** â†’ ë³‘ë ¬í™” ë¶ˆê°€ëŠ¥, GPU ìì› ë‚­ë¹„
- **ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ** â†’ ê¸´ ë¬¸ì¥ì—ì„œ ì´ˆê¸° ì •ë³´ ì†ì‹¤ ë° Gradient Vanishing

> "Recurrence ì—†ì´ Attentionë§Œìœ¼ë¡œ ì¶©ë¶„í•˜ë‹¤"

### Sequence Model ë°œì „ íë¦„

```
~2014              2015                    2017
RNN/LSTM/GRU  â†’  Seq2Seq + Attention  â†’  Transformer
(Sequential)     (Bahdanau et al.)       (Pure Attention)
```

---

## âš ï¸ Background: ê¸°ì¡´ ëª¨ë¸ì˜ í•œê³„

| Model | ì‹œê°„ ë³µì¡ë„ | ê²½ë¡œ ê¸¸ì´ | ë¬¸ì œì  |
|-------|------------|----------|--------|
| RNN | O(n) | O(n) | ìˆœì°¨ ì²˜ë¦¬ ë³‘ëª©, ì¥ê¸° ì˜ì¡´ì„± ì·¨ì•½ |
| CNN | O(log n) | O(log n) | Receptive Field ì œí•œ |
| **Transformer** | **O(1)** | **O(1)** | âœ… í•´ê²° |

---

## ğŸ¯ Motivation & Goal

- âœ… **ì™„ì „í•œ ë³‘ë ¬í™”**: ëª¨ë“  í† í°ì„ ë™ì‹œì— ì²˜ë¦¬í•˜ì—¬ GPU ìì› ê·¹ëŒ€í™”  
- âœ… **ìƒìˆ˜ ì‹œê°„ ì—°ê²°**: ëª¨ë“  ë‹¨ì–´ ê°„ ì§ì ‘ ì—°ê²°(Direct Access)ë¡œ ê²½ë¡œ ìµœì†Œí™”  
- âœ… **ì¥ê¸° ì˜ì¡´ì„± í•´ê²°**: ë¬¸ì¥ ê¸¸ì´ì— ìƒê´€ì—†ì´ ì •ë³´ ì†ì‹¤ ì—†ëŠ” í•™ìŠµ

---

## ğŸ—ï¸ Model Architecture

### ì „ì²´ êµ¬ì¡°

- **Encoder-Decoder Stack**: ê°ê° N=6ê°œì˜ ë™ì¼í•œ ë ˆì´ì–´
- **Multi-Head Attention**: ê¸€ë¡œë²Œ ì˜ì¡´ì„± í¬ì°©
- **Position-wise FFN**: ê° ìœ„ì¹˜ì—ì„œ ë…ë¦½ì ì¸ ë¹„ì„ í˜• ë³€í™˜
- **Residual & LayerNorm**: ê° ì„œë¸Œë ˆì´ì–´ì— Add & Norm ì ìš©
- **Positional Encoding**: Sine/Cosine í•¨ìˆ˜ë¡œ ìœ„ì¹˜ ì •ë³´ ì£¼ì…

---

## ğŸ”‘ Core Mechanism: Scaled Dot-Product Attention

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

| ê¸°í˜¸ | ì˜ë¯¸ |
|------|------|
| Q (Query) | $W^Q \times X$ â€” ì°¾ëŠ” ëŒ€ìƒ |
| K (Key) | $W^K \times X$ â€” ë§¤ì¹­ ê¸°ì¤€ |
| V (Value) | $W^V \times X$ â€” ì •ë³´ ë‚´ìš© |
| $\sqrt{d_k}$ | ìŠ¤ì¼€ì¼ë§ â€” ê¸°ìš¸ê¸° ì•ˆì •í™” |

**ê³„ì‚° íë¦„**: QÂ·Káµ€ ìœ ì‚¬ë„ ê³„ì‚° â†’ âˆšdkë¡œ ìŠ¤ì¼€ì¼ë§ â†’ Softmax â†’ V ê°€ì¤‘í•©

---

## ğŸ§© Multi-Head Attention

ë‹¨ì¼ Attentionì˜ í¸í–¥ ìœ„í—˜ì„ ì™„í™”í•˜ê³ , ë‹¤ì–‘í•œ ê´€ì ì˜ ì •ë³´ë¥¼ ë³‘ë ¬ë¡œ í•™ìŠµí•©ë‹ˆë‹¤.

- **H=8ê°œì˜ Head**: ì„œë¡œ ë‹¤ë¥¸ ë¬¸ë§¥ì  íŠ¹ì§•ì„ ë™ì‹œì— í¬ì°©
- **ì°¨ì› ë¶„í• **: d_model(512)ë¥¼ h=8ë¡œ ë¶„í•  â†’ ê° HeadëŠ” d_k=64 ì°¨ì›ì—ì„œ ì—°ì‚°
- **Ensemble Effect**: ê° Head ì¶œë ¥ì„ Concat í›„ Linearë¡œ í†µí•©

---

## ğŸ“ Positional Encoding

Recurrenceê°€ ì—†ì–´ ìœ„ì¹˜ ì •ë³´ë¥¼ ë³„ë„ë¡œ ì£¼ì…í•´ì•¼ í•©ë‹ˆë‹¤.

$$PE_{(pos,2i)} = \sin\left(\frac{pos}{10000^{2i/d}}\right)$$
$$PE_{(pos,2i+1)} = \cos\left(\frac{pos}{10000^{2i/d}}\right)$$

**Sinusoidalì„ ì„ íƒí•œ ì´ìœ :**
- ìƒëŒ€ì  ìœ„ì¹˜ë¥¼ ì„ í˜• ë³€í™˜ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥
- ê° ì°¨ì›ë§ˆë‹¤ ê³ ìœ í•œ íŒŒì¥ ì£¼ê¸° ë³´ìœ 
- í•™ìŠµ ì‹œí€€ìŠ¤ë³´ë‹¤ ê¸´ ì…ë ¥ë„ ì²˜ë¦¬ ê°€ëŠ¥ (Extrapolation)

---

## âš™ï¸ Training Configuration

| í•­ëª© | Base | Big |
|------|------|-----|
| Optimizer | Adam (Î²â‚=0.9, Îµ=10â»â¹) | ë™ì¼ |
| Dropout | P=0.1 | - |
| Label Smoothing | Îµ=0.1 | - |
| Hardware | 8Ã— NVIDIA P100 | 8Ã— NVIDIA P100 |
| í•™ìŠµ ì‹œê°„ | 12ì‹œê°„ (100K steps) | 3.5ì¼ (300K steps) |
| warmup_steps | 4,000 | - |

---

## ğŸ“Š Experimental Results

### Translation Performance (EN-DE)

| Model | BLEU | Training Cost |
|-------|------|---------------|
| Previous SOTA | 25.16 | - |
| Transformer Base | 27.3 | 12h (8GPU) |
| **Transformer Big â˜…** | **28.4** | 3.5d (8GPU) |

- EN-FR: **41.8 BLEU** (ë‹¹ì‹œ SOTA)
- ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ **1/4 ~ 1/10 ë¹„ìš©**ìœ¼ë¡œ ë‹¬ì„±

### Ablation Study

| ìš”ì†Œ | ê²°ê³¼ |
|------|------|
| Heads h=8 | ìµœì  ì„±ëŠ¥ |
| h=1 ë˜ëŠ” h=16 | -0.9 BLEU ì €í•˜ |
| Positional Encoding ì œê±° | ì„±ëŠ¥ ëŒ€í­ í•˜ë½ |
| Dropout | Overfitting ë°©ì§€ì— í•„ìˆ˜ |

---

## âœ… Strengths

1. **ì™„ì „í•œ ë³‘ë ¬ ì²˜ë¦¬**: RNN O(n) â†’ Transformer O(1)
2. **ì¥ê¸° ì˜ì¡´ì„± í•´ê²°**: ê²½ë¡œ ê¸¸ì´ O(n) â†’ O(1) ì§ì ‘ ì—°ê²°
3. **ë²”ìš©ì„±ê³¼ í™•ì¥ì„±**: NLP(ë²ˆì—­, ìš”ì•½, QA), Vision(ViT, CLIP) ë“± ê´‘ë²”ìœ„ ì ìš©

---

## âš¡ Limitations & Solutions

| í•œê³„ | ì„¤ëª… | í•´ê²°ì±… |
|------|------|--------|
| Quadratic Complexity | ì‹œí€€ìŠ¤ ê¸¸ì´ì— ë”°ë¼ O(nÂ²) ì—°ì‚° ì¦ê°€ | Efficient Attention (Linear) |
| Simple PE | ê³ ì •ëœ Sin/Cos ë°©ì‹ì˜ í‘œí˜„ í•œê³„ | RoPE, ALiBi ë“± ê³ ê¸‰ PE |
| High Resource | ëŒ€ê·œëª¨ í•™ìŠµ ë¹„ìš© (BERT Base: 16 TPU Ã— 4ì¼) | Distillation, Quantization |

---

## ğŸŒ Impact & Future

- **BERT, RoBERTa**: Encoder ê¸°ë°˜, ì–‘ë°©í–¥ ë¬¸ë§¥ ì´í•´
- **GPT-4, ChatGPT**: Decoder-only, ì¸ê°„ ìˆ˜ì¤€ í…ìŠ¤íŠ¸ ìƒì„±
- **ViT, CLIP, DALL-E**: CNN ì—†ì´ ì´ë¯¸ì§€ ì²˜ë¦¬ ê°€ëŠ¥

> "RNN ì—†ëŠ” ì‹œí€€ìŠ¤ ëª¨ë¸ë§" í‘œì¤€ í™•ë¦½ â†’ í˜„ëŒ€ AIì˜ Foundation Architecture

---

## ğŸ“ Files

| íŒŒì¼ | ì„¤ëª… |
|------|------|
| `attention_is_all_you_need.pptx` | ë°œí‘œ ìŠ¬ë¼ì´ë“œ |

---

## ğŸ“š Reference

- Vaswani, A., et al. (2017). *Attention Is All You Need*. NIPS 2017.  
  [arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
